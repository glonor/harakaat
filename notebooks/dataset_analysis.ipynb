{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"arbml/tashkeela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_characters(text1, text2):\n",
    "    # Convert texts into sets of characters\n",
    "    set_text1 = set(text1)\n",
    "    set_text2 = set(text2)\n",
    "\n",
    "    # Find unique characters in each set\n",
    "    unique_in_text1 = set_text1.difference(set_text2)\n",
    "\n",
    "    return unique_in_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set()  # Initialize an empty set to store unique characters\n",
    "n_words = 0  # Initialize a counter for the number of words\n",
    "\n",
    "for example in tqdm(dataset[\"train\"]):\n",
    "    unique_text1 = unique_characters(example[\"diacratized\"], example[\"text\"])\n",
    "    unique_chars.update(unique_text1)\n",
    "    n_words += count_words(example[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of words: {n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Unicode encoding in hexadecimal\n",
    "for char in sorted(unique_chars):\n",
    "    print(f\"Character: {char}, Unicode hexadecimal: {ord(char):x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def strip_diacritics(text):\n",
    "    pattern = re.compile(\"[\\u064B-\\u0652]\")\n",
    "\n",
    "    stripped_text = pattern.sub(\"\", text)\n",
    "\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "text = example[\"diacratized\"]\n",
    "stripped_text = strip_diacritics(text)\n",
    "print(\"Stripped text:\", stripped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check example[\"diacratized\"] and example[\"text\"] are equal after stripping diacritics\n",
    "stripped_text = strip_diacritics(example[\"diacratized\"])\n",
    "stripped_text == example[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_line = example[\"diacratized\"].split()\n",
    "\n",
    "# Iterate over each word pair\n",
    "for original_word in original_line:\n",
    "    # Calculate the size of the resulting sentence if we add the next word\n",
    "    print(original_word)\n",
    "    print(original_word.encode(\"utf-8\"))\n",
    "    print(len(original_word.encode(\"utf-8\")))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "\n",
    "# Tokenize the text\n",
    "encoded = tokenizer.batch_encode_plus(\n",
    "    [original_word], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\n",
    "    example[\"diacratized\"],\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=700,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[encoded == tokenizer.pad_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_diacritics(text):\n",
    "    pattern = re.compile(\"[\\u064b-\\u0652]\")\n",
    "\n",
    "    stripped_text = pattern.sub(\"\", text)\n",
    "\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def tokenize(example, max_len):\n",
    "    target_line = example[\"diacratized\"]\n",
    "\n",
    "    target_line = target_line.split()\n",
    "\n",
    "    new_target_line = \"\"\n",
    "    byte_count = 0\n",
    "\n",
    "    # Iterate over each word pair\n",
    "    for target_word in target_line:\n",
    "        # Calculate the size of the resulting sentence if we add the next word\n",
    "        next_byte_count = byte_count + len(target_word.encode(\"utf-8\"))\n",
    "        if new_target_line:\n",
    "            next_byte_count += len(\" \")\n",
    "\n",
    "        # Check if adding the next word will exceed the byte limit\n",
    "        if next_byte_count > max_len:\n",
    "            break\n",
    "\n",
    "        # Add preceding space only if it's not the first word\n",
    "        if new_target_line:\n",
    "            new_target_line += \" \"\n",
    "        new_target_line += target_word\n",
    "\n",
    "        byte_count = next_byte_count\n",
    "\n",
    "    example[\"diacratized\"] = new_target_line\n",
    "    example[\"text\"] = strip_diacritics(new_target_line)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"arbml/tashkeelav2\")\n",
    "tokenized_dataset = dataset.map(tokenize, fn_kwargs={\"max_len\": 512}, num_proc=6)\n",
    "tokenized_dataset.save_to_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_diacritics(text):\n",
    "    pattern = re.compile(\"[\\u064b-\\u0652]\")\n",
    "\n",
    "    stripped_text = pattern.sub(\"\", text)\n",
    "\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"data\", keep_in_memory=True)\n",
    "\n",
    "for example in dataset[\"test\"]:\n",
    "    stripped_text = strip_diacritics(example[\"diacratized\"])\n",
    "    # assert len(example[\"diacratized\"].split()) == len(example[\"text\"].split())\n",
    "    assert len(example[\"diacratized\"].encode(\"utf-8\")) <= 512\n",
    "    if stripped_text != example[\"text\"]:\n",
    "        print(stripped_text)\n",
    "        print(example[\"text\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"arbml/tashkeelav2\")\n",
    "dataset = dataset.map(tokenize, fn_kwargs={\"max_len\": 100})\n",
    "dataset.save_to_disk(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\n",
    "dataset = load_from_disk(\"data\", keep_in_memory=True)\n",
    "split = \"test\"\n",
    "max_len = 512\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    if batch[\"input_ids\"].shape[1] != max_len:\n",
    "        print(\"input_ids\")\n",
    "        print(batch[\"input_ids\"].shape)\n",
    "        break\n",
    "    if batch[\"attention_mask\"].shape[1] != max_len:\n",
    "        print(\"attention_mask\")\n",
    "        print(batch[\"attention_mask\"].shape)\n",
    "        break\n",
    "    if batch[\"target_ids\"].shape[1] != max_len:\n",
    "        print(\"target_ids\")\n",
    "        print(batch[\"target_ids\"].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(batch[\"target_ids\"][0], skip_special_tokens=True)\n",
    "len(decoded.encode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
